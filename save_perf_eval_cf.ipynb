{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d1291add-4fe9-41f6-903b-f0d2675b38e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0.028001070022583008 sec\n",
      "time: 1.1621155738830566 sec\n",
      "time: 5.606897354125977 sec\n",
      "time: 5.6115148067474365 sec\n",
      "time: 5.62029242515564 sec\n",
      "time: 5.7673423290252686 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py:5047: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196 observations this day\n",
      "time: 6.9049389362335205 sec\n",
      "Upload to Cloud Storage complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 14074.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response Success: spg-stocks/artifacts/performance-data/m1_performance_2023126_pull_time_2023_1_27_12:42:39.csv upload complete. Total time: 14.285sec\n",
      "time: 12.729716300964355 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, time, warnings, random, requests, datetime, pytz, joblib\n",
    "import functools as ft\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from google.cloud import storage\n",
    "from io import BytesIO\n",
    "import pandas_gbq\n",
    "\n",
    "# !pip install pandas_gbq\n",
    "\n",
    "time0 = time.time()\n",
    "pull_time = datetime.datetime.now()\n",
    "pull_time = pull_time.astimezone(pytz.timezone('America/New_York'))\n",
    "pull_time = pull_time.replace(tzinfo=None)\n",
    "now_time = (str(pull_time.year) + '_' +\n",
    "str(pull_time.month) + '_' + \n",
    "str(pull_time.day) + '_' +\n",
    "str(pull_time.hour) + ':'  +\n",
    "str(pull_time.minute) + ':' +\n",
    "str(pull_time.second))\n",
    "\n",
    "time1 = time.time()\n",
    "# gsutil works on Vertex, but not in a Cloud Function...\n",
    "data_bucket_name = 'pmykola-streaming-projects'\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(data_bucket_name)\n",
    "blobs_all = list(bucket.list_blobs())\n",
    "blobs_specific = list(bucket.list_blobs(prefix='spg-stocks/data'))\n",
    "temp_fnames = [blob.name for blob in blobs_specific]\n",
    "datafiles = ['gs://' + data_bucket_name + '/' + fname for fname in temp_fnames]\n",
    "\n",
    "# datafiles = !gsutil ls gs://pmykola-streaming-projects/spg-stocks/data\n",
    "datafiles = [x for x in datafiles if ('auto_data_last_' in x) & ('pull_time' in x)]\n",
    "f'time: {time.time()-time1} sec'\n",
    "\n",
    "time2 = time.time()\n",
    "tempdf = pd.DataFrame(datafiles, columns=['filename'])\n",
    "tempdf['year'] = 0\n",
    "tempdf['month'] = 0\n",
    "tempdf['day'] = 0\n",
    "# display(tempdf.head())\n",
    "# display(tempdf.iloc[0,:])\n",
    "for i in range(tempdf.shape[0]):\n",
    "    tempdf.loc[i,'year'] = tempdf.loc[i,'filename'].split(\"auto_data_last_\",1)[1][:4]\n",
    "    tail = tempdf.loc[i,'filename'].split(\"pull_time_\",1)[1]\n",
    "    tempdf.loc[i,'month'] = tail.split(\"_\",1)[0]\n",
    "    tempdf.loc[i,'day'] = (tail.split(\"_\",1)[1]).split(\"_\",1)[0]\n",
    "    \n",
    "# display(tempdf.head())\n",
    "\n",
    "tempdf['date'] = pd.to_datetime(tempdf[['year', 'month', 'day']])\n",
    "tempdf.sort_values(by='date', inplace=True)\n",
    "# tempdf = tempdf.tail(4)\n",
    "\n",
    "print(f'time: {time.time()-time2} sec')\n",
    "\n",
    "# this part implements for loop to do perfeval for each date in range\n",
    "\n",
    "tempdf = tempdf.tail(4)\n",
    "# display(tempdf)\n",
    "\n",
    "datafiles = list(tempdf.filename)\n",
    "# print(datafiles)\n",
    "df = pd.read_csv(datafiles[0])\n",
    "print(f'time: {time.time()-time2} sec')\n",
    "\n",
    "df_new = pd.DataFrame(columns = df.columns)\n",
    "for file in datafiles:\n",
    "    temp_df = pd.read_csv(file)\n",
    "    df_new = pd.concat([df_new, temp_df], axis=0)\n",
    "print(f'time: {time.time()-time2} sec')\n",
    "\n",
    "# remove duplicates\n",
    "df_new.reset_index(inplace=True, drop=True)\n",
    "df_new.drop_duplicates(inplace=True)\n",
    "df_new.Datetime = pd.to_datetime(df_new.Datetime)\n",
    "df_new.sort_values(by='Datetime')\n",
    "print(f'time: {time.time()-time2} sec')\n",
    "\n",
    "df = df_new\n",
    "df.Datetime = pd.to_datetime(df.Datetime)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.sort_values(by='Datetime')\n",
    "\n",
    "df['time'] = df.Datetime.dt.time\n",
    "df['date'] = df.Datetime.dt.date\n",
    "\n",
    "df = df.fillna(method='ffill')\n",
    "dayclose = df[df.time==datetime.time(15, 58, 0)]\n",
    "dayopen = df[df.time==datetime.time(9, 30, 0)]\n",
    "dayopen.reset_index(drop=True, inplace=True)\n",
    "dayclose.reset_index(drop=True, inplace=True)\n",
    "dayclose.sort_values(by='date')\n",
    "print(f'time: {time.time()-time2} sec')\n",
    "\n",
    "### now i wanna do feature engineering for all assets \n",
    "\n",
    "asset_list = ['Spx', 'Nasdaq', 'Russel', 'EMXC', 'EEMA', 'EEM', 'VTHR']\n",
    "\n",
    "for asset in asset_list:\n",
    "\n",
    "    df[asset + '_ret'] = 100*(df[asset]/df[asset].shift(1)-1)\n",
    "    df['s_' + asset + '_ret_1prd'] = (100*(df[asset]/df[asset].shift(1)-1)).shift(1)\n",
    "    df['s_' + asset + '_ret_2prd'] = (100*(df[asset]/df[asset].shift(2)-1)).shift(1)\n",
    "    df['s_' + asset + '_ret_4prd'] = (100*(df[asset]/df[asset].shift(4)-1)).shift(1)\n",
    "    # display(df.shape, df.head(5))\n",
    "\n",
    "    df.loc[df.time < datetime.time(9, 32, 0), 's_' + asset + '_1prd'] = np.nan\n",
    "    df.loc[df.time < datetime.time(9, 33, 0), 's_' + asset + '_2prd'] = np.nan\n",
    "    df.loc[df.time < datetime.time(9, 35, 0), 's_' + asset + '_4prd'] = np.nan\n",
    "\n",
    "    dayopen.rename(columns={asset:asset+'_open'}, inplace=True)\n",
    "    # dayopen.head()\n",
    "    dayclose.rename(columns={asset:asset+'_close'}, inplace=True)\n",
    "    dayclose_l1 = dayclose.copy()\n",
    "    dayclose_l2 = dayclose.copy()\n",
    "    dayclose_l1[asset+'_close_l1'] = dayclose_l1[asset+'_close'].shift(1)\n",
    "    dayclose_l2[asset+'_close_l2'] = dayclose_l2[asset+'_close'].shift(2)\n",
    "\n",
    "    df = pd.merge(df, dayopen[['date', asset + '_open']], on=['date'], how='left')\n",
    "    df = pd.merge(df, dayclose_l1[['date', asset + '_close_l1']], on=['date'], how='left')\n",
    "    df = pd.merge(df, dayclose_l2[['date', asset + '_close_l2']], on=['date'], how='left')\n",
    "\n",
    "    df['s_' + asset + '_ret_open'] = (100*(df[asset]/df[asset + '_open']-1)).shift(1)\n",
    "    df['s_' + asset + '_ret_close1'] = (100*(df[asset]/df[asset + '_close_l1']-1)).shift(1)\n",
    "    df['s_' + asset + '_ret_close2'] = (100*(df[asset]/df[asset + '_close_l2']-1)).shift(1)\n",
    "\n",
    "    cols_todrop = [x for x in list(df.columns) if asset in x and 'ret' not in x]\n",
    "    df.drop(columns = cols_todrop, inplace=True)\n",
    "print(f'time: {time.time()-time2} sec')\n",
    "\n",
    "### do prediction ###\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket_name='pmykola-streaming-projects'\n",
    "model_path='spg-stocks/artifacts/en_model.pkl'\n",
    "\n",
    "bucket = storage_client.get_bucket(bucket_name)\n",
    "blob = bucket.blob(model_path)\n",
    "model_file = BytesIO()\n",
    "blob.download_to_file(model_file)\n",
    "trained_model=joblib.load(model_file)\n",
    "\n",
    "# df.date.max()\n",
    "this_day = df.loc[df.date == df.date.max()]\n",
    "print(f'{this_day.shape[0]} observations this day')\n",
    "X = this_day.copy()\n",
    "X.drop(columns = ['Datetime',\n",
    "              'time', \n",
    "              'date', \n",
    "              'Spx_ret', \n",
    "              'Nasdaq_ret', \n",
    "              'Russel_ret', \n",
    "              'EEMA_ret', \n",
    "              'EEM_ret', \n",
    "              'EMXC_ret', \n",
    "              'VXUS_ret', \n",
    "              'VTHR_ret'], \n",
    "              inplace=True,\n",
    "              errors = 'ignore')\n",
    "\n",
    "if(X.count().sum() < X.shape[1]):\n",
    "    print(f'''There are {X.shape[1] - X.count().sum()} missing values. \n",
    "          There will be an error''')\n",
    "\n",
    "y = this_day.VTHR_ret\n",
    "y_hat = trained_model.predict(X)\n",
    "\n",
    "model_rmse = mean_squared_error(y, y_hat)\n",
    "constant_rmse = mean_squared_error(y, np.zeros(len(y)))\n",
    "\n",
    "performance = pd.DataFrame([[100*(r2_score(y, y_hat)), model_rmse, constant_rmse, 100*(1-model_rmse/constant_rmse)]], \n",
    "                      columns = ['R2', 'model_rmse', 'constant_rmse', 'rmse_improvement'])\n",
    "performance['date'] = df.date.max()\n",
    "print(f'time: {time.time()-time2} sec')\n",
    "\n",
    "file_name = 'spg-stocks/artifacts/performance-data/' + \\\n",
    "'m1_performance_' + \\\n",
    "str(df.date.max().year) + \\\n",
    "str(df.date.max().month) + \\\n",
    "str(df.date.max().day) + \\\n",
    "'_pull_time_' + \\\n",
    "now_time + \\\n",
    "'.csv'\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket_name = 'pmykola-streaming-projects'\n",
    "BUCKET = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "blob = BUCKET.blob(file_name)\n",
    "blob.upload_from_string(performance.to_csv(), 'text/csv')\n",
    "print('Upload to Cloud Storage complete.')\n",
    "\n",
    "project_id = 'valid-heuristic-369117'\n",
    "bucket_path = 'gs://pmykola-streaming-projects/spg-stocks/artifacts/performance-data/'\n",
    "table_id = 'spg_stocks.daily_performance'\n",
    "\n",
    "performance.rename(columns={'date':'ddate'}, inplace=True)\n",
    "pandas_gbq.to_gbq(performance, table_id, project_id=project_id, if_exists='append')\n",
    "\n",
    "result = ('Success: ' + file_name + ' upload complete. ' + \n",
    "'Total time: ' + str(time.time()-time0)[:6] + 'sec')\n",
    "print('response', result)\n",
    "print(f'time: {time.time()-time2} sec')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m103"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
